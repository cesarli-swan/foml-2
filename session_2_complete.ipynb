{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning: \n",
    "> # Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn import datasets, linear_model, svm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset as dictionary (with some extra metadata)\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Get iris feature data\n",
    "features = iris.data\n",
    "\n",
    "# Get the names of those features\n",
    "feature_labels = iris.feature_names\n",
    "\n",
    "# Get the numeric representation of iris species (our prediction targets)\n",
    "targets = iris.target\n",
    "\n",
    "# Get the names of those species (so we can recognise prediction targets)\n",
    "target_labels = iris.target_names\n",
    "\n",
    "# Load up the dataframe for later\n",
    "df = pd.DataFrame(data= features, columns= feature_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarising the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of samples and features based on the data 'shape'\n",
    "n_samples = features.shape[0]\n",
    "n_features = features.shape[1]\n",
    "n_targets = len(target_labels)\n",
    "shape = features.shape\n",
    "\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"> The input data has the 'shape' {shape}.\")\n",
    "print(f\"> The input data is {n_features}-dimensional.\")\n",
    "print(f\"\\nThat is to say:\")\n",
    "print(f\"> This dataset has {n_samples} iris samples.\")\n",
    "print(f\"> Each sample has {n_features} features.\")\n",
    "print(f\"> The features are:\")\n",
    "for label in feature_labels:\n",
    "    print(f\"\\t. {label}\")\n",
    "print(f\"> Those {n_features} features will belong to 1 of {n_targets} 'classes' or species of iris\")\n",
    "print(f\"> Those classes are:\")\n",
    "for i, label in enumerate(target_labels):\n",
    "    print(f\"\\t. {label} - {(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Features\n",
    "\n",
    "-----\n",
    "**Remember:** The number of _features_ (or _independent variables_) within a dataset determines the _dimensionality_ of the input data. \n",
    "\n",
    "In the case of the Iris dataset, we have 4 dimensions (5 if you include the dependent variable 'species'), which is a little challenging to visualise. \n",
    "\n",
    "There is, however, a way around this. We can peek at _2-dimensional_ slices of this data - in other words, use a scatterplot matrix!\n",
    "\n",
    "Let's begin.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique feature pairs to plot\n",
    "n_pairs = (n_features * (n_features-1)) / 2\n",
    "\n",
    "# Set number of subplot rows \n",
    "n_rows = 2\n",
    "\n",
    "# Set number of subplot columns \n",
    "n_columns = int(n_pairs / n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a figure to contain a list of 'axes'. \n",
    "# Each axis represents an individual subplot.\n",
    "fig, axes = plt.subplots(n_rows, n_columns, figsize = (16,10))\n",
    "\n",
    "# Flatten the list to 1-dimension, simplifying our access\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get a list of tuples for unique pairs of all features\n",
    "# The tuples hold integers: E.g (0,1) to represent a pair of features\n",
    "pairs = list(itertools.combinations(range(n_features), 2))\n",
    "\n",
    "# Count each unique feature pair and grab their indices \n",
    "font_size = 14\n",
    "for index, (i,j) in enumerate(pairs): \n",
    "\n",
    "    # Set scatter plot data for the current feature pair\n",
    "    x_data = features[:,i]\n",
    "    y_data = features[:,j]\n",
    "\n",
    "    # Get the name of those features to annotate the subplot\n",
    "    x_label = feature_labels[i]\n",
    "    y_label = feature_labels[j]\n",
    "    \n",
    "    # Draw a unique subplot for the current pair\n",
    "    ax = axes[index]\n",
    "    ax.scatter(x_data, y_data, c = targets, cmap='viridis')\n",
    "    ax.set_xlabel(x_label, fontsize=font_size)\n",
    "    ax.set_ylabel(y_label, fontsize=font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remembering Linear Regression\n",
    "\n",
    "-----\n",
    "Last week we looked at a type of supervised machine learning where we train a model using continuous numeric data to predict continuous numeric data. This is called a **regression** problem.\n",
    "\n",
    "> This could be something like, using an individual's _salary_ to predict their life expectancy (in minutes, let's say).\n",
    "\n",
    "> Similarly, you could train a model using historic _share values_ to predict the future value of stock over time.\n",
    "\n",
    "Last week we used individual iris features to train a number of _linear regression_ models to predict other iris features.\n",
    "\n",
    "Let's take another look.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a function to prepare data and train based on feature names\n",
    "def train_linear_regression(x, y):\n",
    "    \n",
    "    # Reshape the data into a single column\n",
    "    x_train = x.reshape(-1, 1)\n",
    "    y_train = y.reshape(-1, 1)\n",
    "    \n",
    "    # Build linear model\n",
    "    model = linear_model.LinearRegression()\n",
    "    \n",
    "    # Train using predictor and target data\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Delete the variables so \n",
    "    del x_train, y_train\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of feature pairs so we can access DataFrame columns\n",
    "label_pairs = list(itertools.permutations(feature_labels, 2)) \n",
    "\n",
    "# Calculate the number of unique feature pairs to plot\n",
    "n_pairs = len(label_pairs)\n",
    "\n",
    "# Set number of subplot rows \n",
    "n_rows = 4\n",
    "\n",
    "# Set number of subplot columns \n",
    "n_columns = int(n_pairs / n_rows)\n",
    "\n",
    "# Generate figure to contain plots and axes to represent a list of individual subplots \n",
    "fig, axes = plt.subplots(n_rows, n_columns, figsize = (16,16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Count each unique feature pair and grab their indices  \n",
    "for index, (predictor, target) in enumerate(label_pairs): \n",
    "    \n",
    "    # Convert iris data from Pandas Series to numpy arrays\n",
    "    x_data = df[predictor].values\n",
    "    y_data = df[target].values\n",
    "\n",
    "    # Get a trained model for this feature pair\n",
    "    model = train_linear_regression(x_data, y_data)\n",
    "    \n",
    "    # Get predictions from the linear model\n",
    "    y_pred = model.predict(x_data.reshape(-1, 1))\n",
    "    \n",
    "    # Draw a subplot for each pair\n",
    "    ax = axes[index]\n",
    "    ax.scatter(x_data, y_pred, marker='^', label='Prediction')\n",
    "    ax.scatter(x_data, y_data, marker='x', label='Ground Truth')\n",
    "    ax.set_xlabel(predictor, fontsize=font_size)\n",
    "    ax.set_ylabel(target, fontsize=font_size)\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now, we rarely see accurate predictions for these regression models. \n",
    "\n",
    "Why?\n",
    "\n",
    "Because the data isn't linear (or [linearly seperable](https://en.wikipedia.org/wiki/Linear_separability) - more on that later). Most of these regression lines (the lines of best fit) **do not** fit the data properly!\n",
    "\n",
    " The data sits in these strange, tangled but also disconnected clouds of data points.\n",
    "\n",
    "The further away the ground truth values are to the predicted ones, the higher the error our model incurs, and the less accurate it will be in deployment. \n",
    "\n",
    "In other words, the _function_ between the input feature and output feature is mostly too complex to represent linearly.\n",
    "\n",
    "In fact, most serious prediction tasks in machine learning require a complex function to accurately capture the relationship between input and output data.\n",
    "\n",
    "This is where _non-linear_ models comes in.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "----\n",
    "\n",
    "Polynomial regression is like drawing a curve instead of a straight line to fit your data points. This curve can bend and twist to better match the patterns in your data. \n",
    "\n",
    "Where:\n",
    "\n",
    " $y = mx + b$\n",
    "\n",
    "describes a straight regression line\n",
    "\n",
    "$y = ax^{2} + bx + c$\n",
    "\n",
    "describes a polynomial regession curve.\n",
    "\n",
    "Essentially, it's an extension of linear regression but allows for a more complex, curved relationship between the independent variable and the dependent variable.\n",
    "\n",
    "This, in essence, is all of machine learning - finding the right function to fit a line!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play around with the following **TODOs** to see how the polynomial regression line changes!\n",
    "\n",
    "- How does the degree influence the regression line?\n",
    "- Are any predictors more helpful than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change these index values [0...3] to set up feature indices\n",
    "    # 0 - sepal length\n",
    "    # 1 - sepal width\n",
    "    # 2 - petal length\n",
    "    # 3 - petal width\n",
    "\n",
    "predictor = feature_labels[0]   # Change value to choose predictor feature\n",
    "target = feature_labels[1]      # Change value to choose target feature\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Get a local copy of the predictor feature and target feature\n",
    "X = df[predictor].values\n",
    "y = df[target].values\n",
    "\n",
    "# Reshape X to fit sklearn's requirements\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets (we'll write our own later)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "\n",
    "# TODO: Change this degree to alter the regression curve\n",
    "degree = 2\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Transform the features into polynomial features\n",
    "poly = PolynomialFeatures(degree)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Fit the Linear Regression model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_poly)\n",
    "y_test_pred = model.predict(X_test_poly)\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.scatter(X, y, color='orange', label='Data Points', marker='x')\n",
    "plt.scatter(X_train, y_train_pred, color='red', label='Training Fit', marker='^')\n",
    "plt.scatter(X_test, y_test_pred, color='green', label='Testing Fit', marker='o')\n",
    "\n",
    "# Sort values for a smoother plot line\n",
    "sort_idx = np.argsort(X.flatten())\n",
    "plt.plot(X[sort_idx], np.concatenate([y_train_pred, y_test_pred])[sort_idx], color='grey', label='Polynomial Fit', linestyle='--')\n",
    "\n",
    "plt.xlabel(f'{predictor}', fontsize=font_size)\n",
    "plt.ylabel(f'{target}', fontsize=font_size)\n",
    "plt.title(f'Polynomial Regression (degree = {degree})', fontsize=font_size)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Something we have neglected to address is how we expect our model to behave post-training; that is how it will function \"in the wild\".\n",
    "\n",
    "Ultimately the goal of machine learning is \"generalisation\" so that when we feed a trained model input data that was not used during the training process, it should still give an accurate prediction.\n",
    "\n",
    "Practically, this needs to be simulated when we build our model. The easiest way to do this is by splitting our dataset in two.\n",
    "\n",
    "We call these the **_training set_** and the **_testing set_**.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Split\n",
    "\n",
    "----\n",
    "\n",
    "Making up a training and testing set sufficiently _representative_ of the task we want to model requires some reordering and random sampling of our dataset.\n",
    "\n",
    "Before we start shifting things around, we would like to keep track of which samples belong to which species, or 'class' of iris. Otherwise there's no way of properly validating our trained model.\n",
    "\n",
    "So let's stitch some data together.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining features and labels together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add iris classes (species) to the dataframe\n",
    "# There are 4 columns in the dataframe, we want to add a fifth\n",
    "df.insert(loc= n_features,\n",
    "        column= \"species\",\n",
    "        value= targets)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now that we have a complete dataframe containing features and targets (input and output data), we can start building our training and testing sets.\n",
    "\n",
    "Ideally, we want to use most of our dataset to train the model - otherwise it may not learn the correct relationship between the iris features and the species!\n",
    "\n",
    "We call this the [train-test split](https://builtin.com/data-science/train-test-split).\n",
    "\n",
    "A reasonable approach is to use 70% of the dataset for training, and the remaining 30% for testing. \n",
    "\n",
    "This way we (should) get a good idea of what each species of iris looks like, and still reserve a decent amount of data to test the accuracy of our model.\n",
    "\n",
    "Here's a function to do just that!\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_test_sets(x, train_size=0.7):\n",
    "    \"\"\"\n",
    "    Function takes a complete DataFrame {x}, splitting it into a training set\n",
    "    and a testing set. The size of the training set (and by extension the \n",
    "    testing set) is determined by {train_size}. \n",
    "\n",
    "    Performs a 70/30 train/test split by default.\n",
    "\n",
    "    \"\"\"\n",
    "    # Ensure the dataset is a numpy array.\n",
    "    assert (type(x) == pd.DataFrame), \"X data must be a dataframe!\"\n",
    "    assert (train_size >= 0.0 and train_size <= 1.0), \"Training set size must be between 0 and 1!\"\n",
    "\n",
    "    print(\"Splitting dataset into training and test sets...\\n\")\n",
    "\n",
    "    # Extract values from DataFrame\n",
    "    dataset = x.values\n",
    "\n",
    "    # Determine the number of samples in the dataset.\n",
    "    n_samples = dataset.shape[0]\n",
    "\n",
    "    # Randomly shuffle data to break up the ordered samples\n",
    "    print(f\"First sample in dataset is {dataset[0]}\")\n",
    "    print(\"Shuffling dataset...\")\n",
    "    np.random.shuffle(dataset)\n",
    "    print(f\"First sample in dataset is now {dataset[0]}\")\n",
    "    \n",
    "    # Get the index of the last training sample (based on the train_size)\n",
    "    last_train_sample_idx = int(n_samples * train_size)\n",
    "\n",
    "    # Get first {train_size} percent of the dataset for training\n",
    "    x_train = dataset[:last_train_sample_idx]\n",
    "    \n",
    "\n",
    "    # Get the last {1 - train_size} percent of the dataset for testing\n",
    "    x_test = dataset[last_train_sample_idx:]\n",
    "\n",
    "    print(\"_\"*30)\n",
    "    del dataset, x\n",
    "    # Return bot datasets\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Another thing to consider is whether our training and testing set have an adequate representation of each class within the training and testing sets. \n",
    "\n",
    "Say we have a training set made up of 2/3 of the complete dataset. This training set exclusively contains samples of the _seritosa_ and _versicolor_ iris. \n",
    "\n",
    "Now say we go to test our model using a testing set exclusively made up of _virginica_ samples. It would have never seen these before!\n",
    "\n",
    "What do you think might happen?\n",
    "\n",
    "We call this [_inter-class imbalance_](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data), and it's a big problem in statistical disciplines.\n",
    "\n",
    "Here's a function that checks the class distribution within a given dataset.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class_distribution(dataset, name = 'Dataset'):\n",
    "    \"\"\"\n",
    "    Function finds the unique target values across the 'species' column\n",
    "    (0, 1 or 2) and counts the number of samples that have those unique values.\n",
    "\n",
    "    \"\"\"\n",
    "    # Count sumber of samples within each unique 'species' class\n",
    "    _, counts = np.unique(dataset[:, -1], return_counts= True)\n",
    "    n_samples = dataset.shape[0]\n",
    "\n",
    "    print(f\"{name} has {n_samples} samples.\")\n",
    "    for species, counts in zip(target_labels, counts):\n",
    "        print(f\"> {counts}/{n_samples} are Iris {species}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the training / testing split\n",
    "\n",
    "- How does the train / test split impact predictions?\n",
    "- How small can the training set be and still produce a reasonable model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this value (between 0.0 and 1.0) to see how it impacts performance!\n",
    "training_set_size = 0.8\n",
    "\n",
    "# Call our function to get training and testing sets\n",
    "x_train, x_test =  get_training_test_sets(df, training_set_size)\n",
    "\n",
    "\n",
    "# See how our datasets are broken up by iris species\n",
    "show_class_distribution(df.values)\n",
    "show_class_distribution(x_train, 'Training set')\n",
    "show_class_distribution(x_test, 'Testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Training the Model\n",
    "\n",
    "----\n",
    "\n",
    "Now we're ready to start training our model (the _classifier_). \n",
    "\n",
    "For this we're going to use _Support Vector Machines_. \n",
    "\n",
    "There's a nice [video](https://www.youtube.com/watch?v=efR1C6CvhmE) explaining this if you're interested. (Forgive Josh, he's just excitable.)\n",
    "\n",
    "Without going into too much detail, the SVM projects our data into an even higher dimension. \n",
    "\n",
    "It does this to help draw a _hyper-plane_ that separates the groups of datapoints making up each class of iris (We'll visualise it later). \n",
    "\n",
    "Here's the train function we'll need.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_set, kernel='linear'):\n",
    "    \"\"\"\n",
    "    Function takes training set and uses it to fit the model.\n",
    "    The training set will contain both a set of features (iris measurements)\n",
    "    and a set of labels (their species). These will be seperated into two different\n",
    "    sets, x_data and y_data.\n",
    "\n",
    "    The 'kernel' type changes how the SVM draws boundaries around samples.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check train set is in the correct numpy format\n",
    "    assert (type(train_set) == np.ndarray), \"Training data should be a numpy array!\"\n",
    "\n",
    "    # Get the first four columns (i.e the features) from the training set\n",
    "    x_train = train_set[:, :-1]\n",
    "\n",
    "    # Get the last column (i.e the species 'label') from the training set\n",
    "    y_train = train_set[:, -1]\n",
    "\n",
    "    # Build the model\n",
    "    model = svm.SVC(kernel=kernel)\n",
    "\n",
    "    # Train the model to learn the relationship between \n",
    "    # features {x_train} and labels {y_train}\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Delete the variables here so we don't confuse them in future training runs\n",
    "    del x_train, y_train\n",
    "\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for Testing the Model\n",
    "----\n",
    "\n",
    "Next the test function.\n",
    "\n",
    "This one takes an already trained model and throws our training set at it.\n",
    "\n",
    "Once it's done, we can generate a [_confusion matrix_](https://en.wikipedia.org/wiki/Confusion_matrix) to see how well the model did.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_set):\n",
    "    \"\"\"\n",
    "    Function makes a trained model and a testing set.\n",
    "    The model uses the test set as 'unseen' input data,\n",
    "    to validate the accuracy of its predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get and reshape the features and labels from the testing set\n",
    "    x_test = test_set[:, :-1].reshape(-1, n_features)\n",
    "    y_test = test_set[:, -1].reshape(-1, 1)\n",
    "    print(f'Test set has {x_test.shape[0]} samples.')\n",
    "\n",
    "    # Get 'species' predictions from the model\n",
    "    print('Predicting. . .')\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Generate a confusion matrix given the test data and predicted data\n",
    "    print('Generating confusion matrix. . .')\n",
    "    c_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    del x_test, y_test\n",
    "\n",
    "    return c_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    \"\"\"\"\n",
    "    Plot the confusion matrix generated from the model predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    # Clear any existing figures\n",
    "    plt.clf()\n",
    "\n",
    "    # Set the figure size before plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Set a custom font size for the confusion matrix\n",
    "    font_size = 14\n",
    "\n",
    "    # Set the font size for all axes labels\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontsize(font_size)\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_labels)\n",
    "\n",
    "    # Colour the confusion matrix\n",
    "    disp.plot(cmap=plt.cm.Reds, ax=ax)\n",
    "\n",
    "    # Set the fontsize for values within the matrix\n",
    "    for text in disp.text_.ravel():\n",
    "        text.set_fontsize(font_size)\n",
    "\n",
    "    # Add title and axes labels \n",
    "    plt.title(\"Confusion Matrix for Iris Classifier\", fontsize= font_size)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=font_size)\n",
    "    plt.ylabel(\"True Label\", fontsize=font_size)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Testing and Evaluations\n",
    "-----\n",
    "\n",
    "We have the functions to train our own SVM model. The only thing left to do is call them. \n",
    "\n",
    "We'll need to feed in our training set then use our test set against that models prediction to generate the confusion matrix.\n",
    "\n",
    "The confusion matrix is grid with the same number of rows and columns.\n",
    "\n",
    "Each row represents the ground truth - what the model should have guessed. The columns represents what the model actually predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose the kernel type to change how the model draws decision boundaries between our training samples\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "kernel =  kernels[0]\n",
    "\n",
    "# Traing the model using the training set \n",
    "model = train(x_train, kernel)\n",
    "\n",
    "# Generate model predictions using the testing set \n",
    "# and return the confusion matrix\n",
    "conf_matrix = test(model, x_test)\n",
    "\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insepcting Performance Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:, -1]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define kernel types\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# Create a 2x2 grid for subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Flatten the axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over each kernel type\n",
    "for i, kernel in enumerate(kernels):\n",
    "    # Train SVM model with current kernel type\n",
    "    svm_model = svm.SVC(kernel=kernel, gamma='scale')\n",
    "    svm_model.fit(X_scaled, y)\n",
    "\n",
    "    # Plot confusion matrix in current subplot\n",
    "    ConfusionMatrixDisplay.from_estimator(svm_model, X_scaled, y, ax=axes[i], cmap=plt.cm.Reds, normalize='true')\n",
    "    axes[i].set_title(f'Confusion Matrix - {kernel.capitalize()} Kernel', fontsize=font_size)\n",
    "    axes[i].set_xlabel('Predicted Class', fontsize=font_size)\n",
    "    axes[i].set_ylabel('Ground Truth', fontsize=font_size)\n",
    "    \n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Hyper-Planes Across Models \n",
    "\n",
    "----\n",
    "\n",
    "To simplify the visualisation, this example uses two features, instead of four, to solve the classification problem.\n",
    "\n",
    "For each kernel type supported by the SVM, we use these two features to train the model and insepct how it draws a decision boundary to distringuish between classes of datapoints.\n",
    "\n",
    "Play around with the **TODOs** to see how things change\n",
    "\n",
    "- Do certain feature pairs make prediction harder?\n",
    "- How do the contours change depending on these features?\n",
    "- Are there any classes harder to distinguish from the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((n_samples, 2))\n",
    "\n",
    "# TODO: Change the features to view different hyperplanes\n",
    "    # 0 - sepal length\n",
    "    # 1 - sepal width\n",
    "    # 2 - petal length\n",
    "    # 3 - petal width\n",
    "\n",
    "x1 = feature_labels[0]      # TODO: Change X feature\n",
    "x2 = feature_labels[3]      # TODO: Change Y feature\n",
    "    \n",
    "X[:, 0] = df[x1].values  \n",
    "X[:, 1] = df[x2].values   \n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "y = df.values[:, -1]\n",
    "\n",
    "# Standardise the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a matrix of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Flatten the axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Iterate over each kernel type\n",
    "for i, kernel in enumerate(kernels):\n",
    "    # Train SVM model with current kernel type\n",
    "    svm_model = svm.SVC(kernel=kernel, gamma='scale')\n",
    "    svm_model.fit(X_scaled, y)\n",
    "\n",
    "    # Create a meshgrid to plot decision surface\n",
    "    x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    # Make predictions on meshgrid points\n",
    "    Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # Plot decision surface in current subplot\n",
    "\n",
    "    font_size = 18\n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "    axes[i].scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
    "    axes[i].set_title(f'SVM Decision Surface with {kernel.capitalize()} Kernel', fontsize=font_size)\n",
    "    axes[i].set_xlabel('Petal length (standardised)', fontsize=font_size)\n",
    "    axes[i].set_ylabel('Petal width (standardised)', fontsize=font_size)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
